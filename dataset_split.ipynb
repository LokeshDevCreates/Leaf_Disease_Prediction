{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c182c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\perum\\appdata\\roaming\\python\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\perum\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\perum\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\perum\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\perum\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\perum\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56d2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b452ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset split and CSV files created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "original_dataset = \"PlantVillage\"  # the folder you extracted\n",
    "base_dir = \"dataset\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# Create directories if not exist\n",
    "for split in [train_dir, test_dir]:\n",
    "    os.makedirs(split, exist_ok=True)\n",
    "\n",
    "# To store CSV data\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# Loop through each class (disease/healthy type)\n",
    "for class_folder in os.listdir(original_dataset):\n",
    "    class_path = os.path.join(original_dataset, class_folder)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    # Create subfolders in train and test\n",
    "    os.makedirs(os.path.join(train_dir, class_folder), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, class_folder), exist_ok=True)\n",
    "\n",
    "    # List images\n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # 80/20 split\n",
    "    split_index = int(0.8 * len(images))\n",
    "    train_files = images[:split_index]\n",
    "    test_files = images[split_index:]\n",
    "\n",
    "    # Move + add to CSV list\n",
    "    for file in train_files:\n",
    "        src = os.path.join(class_path, file)\n",
    "        dst = os.path.join(train_dir, class_folder, file)\n",
    "        shutil.copy(src, dst)\n",
    "        train_data.append([dst, class_folder])  # filepath, label\n",
    "\n",
    "    for file in test_files:\n",
    "        src = os.path.join(class_path, file)\n",
    "        dst = os.path.join(test_dir, class_folder, file)\n",
    "        shutil.copy(src, dst)\n",
    "        test_data.append([dst, class_folder])  # filepath, label\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(train_data, columns=[\"filepath\", \"label\"])\n",
    "test_df = pd.DataFrame(test_data, columns=[\"filepath\", \"label\"])\n",
    "\n",
    "# Save CSV files\n",
    "train_df.to_csv(\"train_labels.csv\", index=False)\n",
    "test_df.to_csv(\"test_labels.csv\", index=False)\n",
    "\n",
    "print(\"✅ Dataset split and CSV files created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514518e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow pillow matplotlib scikit-learn --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35dedcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_DIR = \"dataset\"   # contains train/ and test/\n",
    "TRAIN_CSV = \"train_labels.csv\"\n",
    "TEST_CSV  = \"test_labels.csv\"\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea26f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 16505  | Test rows: 4134\n",
      "Classes (train): 15\n",
      "label\n",
      "Tomato__Tomato_YellowLeaf__Curl_Virus          2567\n",
      "Tomato_Bacterial_spot                          1701\n",
      "Tomato_Late_blight                             1527\n",
      "Tomato_Septoria_leaf_spot                      1416\n",
      "Tomato_Spider_mites_Two_spotted_spider_mite    1340\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# your CSV has columns: filepath, label\n",
    "train_df[\"filepath\"] = train_df[\"filepath\"].apply(lambda p: os.path.abspath(p))\n",
    "test_df[\"filepath\"]  = test_df[\"filepath\"].apply(lambda p: os.path.abspath(p))\n",
    "\n",
    "# quick sanity checks\n",
    "print(\"Train rows:\", len(train_df), \" | Test rows:\", len(test_df))\n",
    "print(\"Classes (train):\", train_df[\"label\"].nunique())\n",
    "print(train_df[\"label\"].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738ab25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13204 validated image filenames belonging to 15 classes.\n",
      "Found 3301 validated image filenames belonging to 15 classes.\n",
      "Found 4133 validated image filenames belonging to 15 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perum\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 1 invalid image filename(s) in x_col=\"filepath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameIterator' object has no attribute 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     32\u001b[39m val_gen = valid_datagen.flow_from_dataframe(\n\u001b[32m     33\u001b[39m     dataframe=train_df,\n\u001b[32m     34\u001b[39m     x_col=\u001b[33m\"\u001b[39m\u001b[33mfilepath\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m test_gen = test_datagen.flow_from_dataframe(\n\u001b[32m     44\u001b[39m     dataframe=test_df,\n\u001b[32m     45\u001b[39m     x_col=\u001b[33m\"\u001b[39m\u001b[33mfilepath\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     51\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m num_classes = \u001b[43mtrain_gen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\n\u001b[32m     54\u001b[39m class_indices = train_gen.class_indices\n\u001b[32m     55\u001b[39m index_to_label = {v:k \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m class_indices.items()}\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrameIterator' object has no attribute 'num_classes'"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.2,          # 20% of train_df becomes validation\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_gen = valid_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_gen.num_classes\n",
    "class_indices = train_gen.class_indices\n",
    "index_to_label = {v:k for k,v in class_indices.items()}\n",
    "print(\"Classes:\", index_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dcb7ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing bad rows:\n",
      " - train: 0\n",
      "- test:  1\n",
      "After cleaning → train: 16505  test: 4133\n",
      "Found 13204 validated image filenames belonging to 15 classes.\n",
      "Found 3301 validated image filenames belonging to 15 classes.\n",
      "Found 4133 validated image filenames belonging to 15 classes.\n",
      "Classes: {0: 'Pepper__bell___Bacterial_spot', 1: 'Pepper__bell___healthy', 2: 'Potato___Early_blight', 3: 'Potato___Late_blight', 4: 'Potato___healthy', 5: 'Tomato_Bacterial_spot', 6: 'Tomato_Early_blight', 7: 'Tomato_Late_blight', 8: 'Tomato_Leaf_Mold', 9: 'Tomato_Septoria_leaf_spot', 10: 'Tomato_Spider_mites_Two_spotted_spider_mite', 11: 'Tomato__Target_Spot', 12: 'Tomato__Tomato_YellowLeaf__Curl_Virus', 13: 'Tomato__Tomato_mosaic_virus', 14: 'Tomato_healthy'}\n",
      "num_classes: 15\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "SEED = 42\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 1) Load CSVs\n",
    "train_df = pd.read_csv(\"train_labels.csv\")\n",
    "test_df  = pd.read_csv(\"test_labels.csv\")\n",
    "\n",
    "# 2) Make absolute paths (Windows-safe) + ensure strings\n",
    "train_df[\"filepath\"] = train_df[\"filepath\"].astype(str).apply(os.path.abspath)\n",
    "test_df[\"filepath\"]  = test_df[\"filepath\"].astype(str).apply(os.path.abspath)\n",
    "train_df[\"label\"] = train_df[\"label\"].astype(str)\n",
    "test_df[\"label\"]  = test_df[\"label\"].astype(str)\n",
    "\n",
    "# 3) Drop bad / missing / non-image files\n",
    "valid_ext = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"}\n",
    "def is_good(p):\n",
    "    return isinstance(p, str) and os.path.exists(p) and os.path.splitext(p)[1].lower() in valid_ext\n",
    "\n",
    "bad_train = train_df[~train_df[\"filepath\"].apply(is_good)]\n",
    "bad_test  = test_df[~test_df[\"filepath\"].apply(is_good)]\n",
    "if not bad_train.empty or not bad_test.empty:\n",
    "    print(\"Removing bad rows:\\n\",\n",
    "          f\"- train: {len(bad_train)}\\n- test:  {len(bad_test)}\")\n",
    "train_df = train_df[train_df[\"filepath\"].apply(is_good)].drop_duplicates(\"filepath\")\n",
    "test_df  = test_df[test_df[\"filepath\"].apply(is_good)].drop_duplicates(\"filepath\")\n",
    "\n",
    "print(\"After cleaning → train:\", len(train_df), \" test:\", len(test_df))\n",
    "\n",
    "# 4) Build generators (with internal validation split from train_df)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.20,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.20\n",
    ")\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "val_gen = valid_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n",
    "test_gen = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 5) FIX: derive num_classes from class_indices (works in Keras 3 legacy)\n",
    "class_indices = train_gen.class_indices\n",
    "num_classes = len(class_indices)\n",
    "index_to_label = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "print(\"Classes:\", index_to_label)\n",
    "print(\"num_classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d3378a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: np.float64(1.3805938937682978),\n",
       " 1: np.float64(0.9309080654258319),\n",
       " 2: np.float64(1.3754166666666667),\n",
       " 3: np.float64(1.3754166666666667),\n",
       " 4: np.float64(9.09366391184573),\n",
       " 5: np.float64(0.6468743876151284),\n",
       " 6: np.float64(1.3754166666666667),\n",
       " 7: np.float64(0.7205850251036892),\n",
       " 8: np.float64(1.445904511607534),\n",
       " 9: np.float64(0.7770715630885122),\n",
       " 10: np.float64(0.8211442786069652),\n",
       " 11: np.float64(0.9798159691303058),\n",
       " 12: np.float64(0.4286456304376055),\n",
       " 13: np.float64(3.692393736017897),\n",
       " 14: np.float64(0.865041928721174)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_array = train_df[\"label\"].values\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels_array),\n",
    "    y=labels_array\n",
    ")\n",
    "# map from class index (0..num_classes-1) to weight\n",
    "class_weight_dict = {class_indices[c]: w for c, w in zip(np.unique(labels_array), class_weights)}\n",
    "class_weight_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30d63218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,215</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │        \u001b[38;5;34m19,215\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,277,199</span> (8.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,277,199\u001b[0m (8.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,215</span> (75.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,215\u001b[0m (75.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = MobileNetV2(\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base_model.trainable = False  # freeze for initial training\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de6b682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 1s/step - accuracy: 0.7122 - loss: 0.8801 - val_accuracy: 0.1039 - val_loss: 9.4209\n",
      "Epoch 2/10\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 2s/step - accuracy: 0.8304 - loss: 0.5181 - val_accuracy: 0.1109 - val_loss: 10.2544\n",
      "Epoch 3/10\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 2s/step - accuracy: 0.8501 - loss: 0.4478 - val_accuracy: 0.1121 - val_loss: 10.8347\n",
      "Epoch 4/10\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1382s\u001b[0m 3s/step - accuracy: 0.8610 - loss: 0.4150 - val_accuracy: 0.1094 - val_loss: 11.2011\n",
      "Epoch 5/10\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 2s/step - accuracy: 0.8664 - loss: 0.3910 - val_accuracy: 0.1154 - val_loss: 11.8385\n",
      "Epoch 6/10\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 2s/step - accuracy: 0.8682 - loss: 0.3867 - val_accuracy: 0.1148 - val_loss: 11.9365\n",
      "Epoch 7/10\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 2s/step - accuracy: 0.8719 - loss: 0.3731 - val_accuracy: 0.1112 - val_loss: 12.2799\n",
      "Epoch 8/10\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m904s\u001b[0m 2s/step - accuracy: 0.8761 - loss: 0.3592 - val_accuracy: 0.1127 - val_loss: 12.6792\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=3, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "    ModelCheckpoint(\"leaf_disease_model.keras\", save_best_only=True, monitor=\"val_accuracy\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b36669",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# unfreeze last ~30 layers of the base model for fine-tuning\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbase_model\u001b[49m.layers[-\u001b[32m30\u001b[39m:]:\n\u001b[32m      3\u001b[39m     layer.trainable = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      5\u001b[39m model.compile(\n\u001b[32m      6\u001b[39m     optimizer=tf.keras.optimizers.Adam(learning_rate=\u001b[32m1e-4\u001b[39m),\n\u001b[32m      7\u001b[39m     loss=\u001b[33m\"\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     metrics=[\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "# unfreeze last ~30 layers of the base model for fine-tuning\n",
    "for layer in base_model.layers[-30:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict  # remove if not using class weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ea0f1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 1s/step - accuracy: 0.7293 - loss: 2.6337\n",
      "Test accuracy: 72.93%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_gen)\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ebdccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m835s\u001b[0m 2s/step - accuracy: 0.8623 - loss: 0.4022 - val_accuracy: 0.0827 - val_loss: 17.0355\n",
      "Epoch 2/5\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m889s\u001b[0m 2s/step - accuracy: 0.9293 - loss: 0.1847 - val_accuracy: 0.1030 - val_loss: 19.8397\n",
      "Epoch 3/5\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 2s/step - accuracy: 0.9504 - loss: 0.1229 - val_accuracy: 0.1066 - val_loss: 19.8369\n"
     ]
    }
   ],
   "source": [
    "# unfreeze last ~30 layers of the base model for fine-tuning\n",
    "for layer in base_model.layers[-30:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict  # remove if not using class weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0cef27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 1s/step - accuracy: 0.6731 - loss: 3.8785\n",
      "Test accuracy: 67.31%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_gen)\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8bc605b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'label_map.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model = load_model(\u001b[33m\"\u001b[39m\u001b[33mleaf_disease_model.keras\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Reload label map\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel_map.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      9\u001b[39m     index_to_label = json.load(f)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel and label map reloaded ✅\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'label_map.json'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "# Load the saved head-only model\n",
    "model = load_model(\"leaf_disease_model.keras\")\n",
    "\n",
    "# Reload label map\n",
    "with open(\"label_map.json\", \"r\") as f:\n",
    "    index_to_label = json.load(f)\n",
    "\n",
    "print(\"Model and label map reloaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f80e71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: label_map.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Make sure train_gen is already defined from earlier cells\n",
    "class_indices = train_gen.class_indices\n",
    "index_to_label = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "with open(\"label_map.json\", \"w\") as f:\n",
    "    json.dump(index_to_label, f, indent=2)\n",
    "\n",
    "print(\"Saved: label_map.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f281241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and label map reloaded ✅\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "model = load_model(\"leaf_disease_model.keras\")\n",
    "\n",
    "with open(\"label_map.json\", \"r\") as f:\n",
    "    index_to_label = json.load(f)\n",
    "\n",
    "print(\"Model and label map reloaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "775f646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 953ms/step - accuracy: 0.7293 - loss: 2.6337\n",
      "Test accuracy (reloaded model): 72.93%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_gen)\n",
    "print(f\"Test accuracy (reloaded model): {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02897a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2318bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: leaf_disease_model.keras and label_map.json\n"
     ]
    }
   ],
   "source": [
    "# model file already saved by ModelCheckpoint as leaf_disease_model.keras\n",
    "with open(\"label_map.json\", \"w\") as f:\n",
    "    json.dump(index_to_label, f, indent=2)\n",
    "\n",
    "print(\"Saved: leaf_disease_model.keras and label_map.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20d65cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "np.int64(6)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Try it on one test image\u001b[39;00m\n\u001b[32m     13\u001b[39m sample_path = test_df.iloc[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfilepath\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m pred, p = \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction:\u001b[39m\u001b[33m\"\u001b[39m, pred, \u001b[33m\"\u001b[39m\u001b[33m| confidence:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(p, \u001b[32m3\u001b[39m))\n\u001b[32m     16\u001b[39m sample_path\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mpredict_image\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      8\u001b[39m probs = model.predict(arr)\n\u001b[32m      9\u001b[39m idx = np.argmax(probs[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindex_to_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(probs[\u001b[32m0\u001b[39m][idx])\n",
      "\u001b[31mKeyError\u001b[39m: np.int64(6)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "def predict_image(path):\n",
    "    img = load_img(path, target_size=IMG_SIZE)\n",
    "    arr = img_to_array(img)\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    arr = preprocess_input(arr)\n",
    "    probs = model.predict(arr)\n",
    "    idx = np.argmax(probs[0])\n",
    "    return index_to_label[idx], float(probs[0][idx])\n",
    "\n",
    "# Try it on one test image\n",
    "sample_path = test_df.iloc[0][\"filepath\"]\n",
    "pred, p = predict_image(sample_path)\n",
    "print(\"Prediction:\", pred, \"| confidence:\", round(p, 3))\n",
    "sample_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50a7cfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: leaf_disease_model.keras and label_map.json\n"
     ]
    }
   ],
   "source": [
    "# model file already saved by ModelCheckpoint as leaf_disease_model.keras\n",
    "\n",
    "# ensure keys are integers before saving\n",
    "index_to_label_int = {int(v): k for v, k in index_to_label.items()}\n",
    "\n",
    "with open(\"label_map.json\", \"w\") as f:\n",
    "    json.dump(index_to_label_int, f, indent=2)\n",
    "\n",
    "print(\"Saved: leaf_disease_model.keras and label_map.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e56ca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "np.int64(6)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[32m     14\u001b[39m sample_path = test_df.iloc[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfilepath\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m pred, p = \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction:\u001b[39m\u001b[33m\"\u001b[39m, pred, \u001b[33m\"\u001b[39m\u001b[33m| confidence:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(p, \u001b[32m3\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mpredict_image\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      9\u001b[39m probs = model.predict(arr)\n\u001b[32m     10\u001b[39m idx = np.argmax(probs[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindex_to_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(probs[\u001b[32m0\u001b[39m][idx])\n",
      "\u001b[31mKeyError\u001b[39m: np.int64(6)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "def predict_image(path):\n",
    "    img = load_img(path, target_size=IMG_SIZE)\n",
    "    arr = img_to_array(img)\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    arr = preprocess_input(arr)\n",
    "    probs = model.predict(arr)\n",
    "    idx = np.argmax(probs[0])\n",
    "    return index_to_label[idx], float(probs[0][idx])\n",
    "\n",
    "# Try again\n",
    "sample_path = test_df.iloc[0][\"filepath\"]\n",
    "pred, p = predict_image(sample_path)\n",
    "print(\"Prediction:\", pred, \"| confidence:\", round(p, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a4e9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "\n",
    "model = load_model(\"leaf_disease_model.keras\")\n",
    "\n",
    "with open(\"label_map.json\", \"r\") as f:\n",
    "    index_to_label = json.load(f)   # already has integer keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32498a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "np.int64(6)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[32m     14\u001b[39m sample_path = test_df.iloc[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfilepath\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m pred, p = \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction:\u001b[39m\u001b[33m\"\u001b[39m, pred, \u001b[33m\"\u001b[39m\u001b[33m| confidence:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(p, \u001b[32m3\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mpredict_image\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      9\u001b[39m probs = model.predict(arr)\n\u001b[32m     10\u001b[39m idx = np.argmax(probs[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindex_to_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(probs[\u001b[32m0\u001b[39m][idx])\n",
      "\u001b[31mKeyError\u001b[39m: np.int64(6)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "def predict_image(path):\n",
    "    img = load_img(path, target_size=IMG_SIZE)\n",
    "    arr = img_to_array(img)\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    arr = preprocess_input(arr)\n",
    "    probs = model.predict(arr)\n",
    "    idx = np.argmax(probs[0])\n",
    "    return index_to_label[idx], float(probs[0][idx])\n",
    "\n",
    "# Try again\n",
    "sample_path = test_df.iloc[0][\"filepath\"]\n",
    "pred, p = predict_image(sample_path)\n",
    "print(\"Prediction:\", pred, \"| confidence:\", round(p, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c05eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_to_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Ensure label map keys are integers (important for lookups)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m index_to_label = {\u001b[38;5;28mint\u001b[39m(k): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mindex_to_label\u001b[49m.items()}\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_image\u001b[39m(path):\n\u001b[32m      8\u001b[39m     img = load_img(path, target_size=IMG_SIZE)   \u001b[38;5;66;03m# load image\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'index_to_label' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# Ensure label map keys are integers (important for lookups)\n",
    "index_to_label = {int(k): v for k, v in index_to_label.items()}\n",
    "\n",
    "def predict_image(path):\n",
    "    img = load_img(path, target_size=IMG_SIZE)   # load image\n",
    "    arr = img_to_array(img)                      # convert to array\n",
    "    arr = np.expand_dims(arr, axis=0)            # add batch dimension\n",
    "    arr = preprocess_input(arr)                  # preprocess for MobileNetV2\n",
    "    probs = model.predict(arr)                   # model prediction\n",
    "    idx = np.argmax(probs[0])                    # predicted class index\n",
    "    return index_to_label[idx], float(probs[0][idx])\n",
    "\n",
    "# Try it on one test image\n",
    "sample_path = test_df.iloc[0][\"filepath\"]\n",
    "pred, p = predict_image(sample_path)\n",
    "print(\"Prediction:\", pred, \"| confidence:\", round(p, 3))\n",
    "sample_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6e83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542be1b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Pick 5 random test images\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sample_paths = random.sample(\u001b[38;5;28mlist\u001b[39m(\u001b[43mtest_df\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mfilepath\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[32m5\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m sample_paths:\n\u001b[32m      7\u001b[39m     pred, p = predict_image(path)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Pick 5 random test images\n",
    "sample_paths = random.sample(list(test_df[\"filepath\"]), 5)\n",
    "\n",
    "for path in sample_paths:\n",
    "    pred, p = predict_image(path)\n",
    "    print(f\"File: {os.path.basename(path)}\")\n",
    "    print(f\" → Prediction: {pred} | Confidence: {round(p, 3)}\")\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54008d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate on training data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_loss, train_acc = \u001b[43mmodel\u001b[49m.evaluate(X_train, y_train, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Evaluate on test/validation data\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate on training data\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Evaluate on test/validation data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a27e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perum\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 36 variables whereas the saved optimizer has 70 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"plant_disease_model.keras\")   # load your trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ece94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)   # MobileNetV2 default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a614ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27298 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=None)  # same preprocessing as training\n",
    "train_gen_tmp = datagen.flow_from_directory(\n",
    "    \"dataset\",   # 👈 your training dataset folder\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_indices = train_gen_tmp.class_indices\n",
    "index_to_label = {v: k for k, v in class_indices.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aee5cb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 0, 'train': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_gen_tmp.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d7d880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'test', 1: 'train'}\n"
     ]
    }
   ],
   "source": [
    "index_to_label = {v: k for k, v in train_gen_tmp.class_indices.items()}\n",
    "print(index_to_label)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18d60248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7463 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMG_SIZE = (224, 224)  # must match training size\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=None  # use the same preprocessing as training if needed\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    \"dataset/test\",   # 👈 replace with your test folder path\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc56152f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1711s\u001b[0m 7s/step - accuracy: 0.0722 - loss: 6.5113\n",
      "Test Accuracy: 0.0722\n",
      "Test Loss: 6.5113\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "print(f\"Test Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f445c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perum\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 36 variables whereas the saved optimizer has 70 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7463 images belonging to 15 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perum\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1552s\u001b[0m 7s/step - accuracy: 0.9433 - loss: 0.1673\n",
      "Test Loss: 0.1673\n",
      "Test Accuracy: 0.9433\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 1. Load your fine-tuned model\n",
    "model = load_model(\"plant_disease_model.keras\")\n",
    "\n",
    "# 2. Use the SAME preprocessing as in Colab\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 3. Point this to your test dataset folder\n",
    "test_dir = \"dataset/test\"   # <-- change this to your test folder path\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),   # must match training image_size\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False             # important for evaluation\n",
    ")\n",
    "\n",
    "# 4. Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a776b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m234/234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1187s\u001b[0m 5s/step - accuracy: 0.9433 - loss: 0.1673\n",
      "Test Loss: 0.1673, Test Accuracy: 0.9433\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Save metrics\n",
    "import json\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    json.dump({\"loss\": float(loss), \"accuracy\": float(acc)}, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
